---
title: "Richard Moley Article Scraping"
author: "Bridget Lang"
date: "2024-11-16"
output: html_document
---
```{r}
library(tidyverse)
library(pdftools)
library(dplyr)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
#install.packages("pdftools")
```

#Make a dataframe of all the names of the pdf files 

```{r}
#from: https://stackoverflow.com/a/68032558
getMatch = function(rexp, str) regmatches(str, regexpr(rexp, str))

article_index <- list.files("./moley_newsweek_2", pattern="*txt") %>% 
  as.data.frame() |>
  rename(filename = 1) |> 
  mutate(index = row_number()) 

#get date from file name
article_index <- article_index %>%
  mutate(date = getMatch("[0-9]{4}-[0-9]{2}-[0-9]{2}",filename))

#extract year into own column
article_index <- article_index %>%
  mutate(year = getMatch("[0-9]{4}", date))

#extract month into own column
article_index <- article_index %>%
  mutate(month = str_replace_all(getMatch("-[0-9]{2}-", date), "-", ""))

#extract day into own column
article_index <- article_index %>%
  mutate(day = str_replace_all(str_replace_all(getMatch("-[0-9]{2}_", filename), "-", ""), "_", ""))

article_index <- article_index %>%
  select(index, filename, date, year, month, day)

write.csv(article_index, "article_index.csv")

article_index
```

# Compiling text
```{r}
create_article_text <- function(filename) {
  
    articles_df_temp <- read_lines(glue::glue("./moley_newsweek/{filename}"))%>%
    as_tibble() %>%
    mutate(filename = filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()

sapply(article_index$filename, create_article_text)

```
# Break up "value" column into lines of length 12
```{r}

articles_df$value <- iconv(articles_df$value, from = "", to = "UTF-8", sub = "")  # Replace invalid characters with ""

# Additional step to remove non-printable and problematic characters
articles_df$value <- gsub("[^[:print:]]+", " ", articles_df$value)  # Remove non-printable characters
articles_df$value <- gsub("\\s+", " ", trimws(articles_df$value))  # Normalize whitespace
# Function to split text into chunks of 12 words

split_into_chunks <- function(text, n = 12) {
  words <- unlist(strsplit(text, "\\s+"))  # Split text into words
  split_words <- split(words, ceiling(seq_along(words) / n))  # Group words into chunks
  chunks <- sapply(split_words, paste, collapse = " ")  # Combine words back into strings
  return(chunks)
}

# Process the dataframe
library(dplyr)

lines <- articles_df %>%
  rowwise() %>%
  mutate(
    text_chunks = list(split_into_chunks(value))
  ) %>%
  unnest(text_chunks) %>%
  select(filename, text_chunks) %>%
  rename(sentence = text_chunks)

lines <- lines %>%
  inner_join(article_index, c("filename")) 
lines <- lines %>%
  select(sentence, date, year, month, day, filename)
# Print the result


write.csv(lines, "articles_lines.csv")
```

# Make a dataframe with one word per row
```{r}
article_text <-  read.csv("./articles_lines.csv")

data(stop_words)
one_word_per_row <- article_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "- ", "")) %>% 
  unnest_tokens(word, text, token="ngrams", n=1 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!is.na(word))
one_word_per_row
```
# Create bigrams and clean them
```{r}
bigrams <- article_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "raymond", "")) %>% 
  mutate(text = str_replace_all(text, "demo", "")) %>% 
  mutate(text = str_replace_all(text, "crats", "")) %>% 
  unnest_tokens(word, text, token="ngrams", n=2 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!is.na(word))

bigrams <- bigrams %>%
  select(word, date, year, month, day, filename)

bigrams
```

# Cleaning bigrams
```{r}
bigrams_separated <- bigrams %>%
  separate(word, c("word1", "word2"), sep = " ")

#bigrams with stop words filtered

bigrams_filtered <- 
  bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

write.csv(bigrams_filtered, "bigrams_filtered.csv")

bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigram_counts

write.csv(bigram_counts, "bigram_counts.csv")

  
top_20_bigrams <- bigram_counts |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)

top_20_bigrams
```